"""
Sistema Avanzado de An√°lisis de Texto con Procesamiento Concurrente
Demuestra: decoradores, context managers, generators, async/await, metaclases,
descriptores, threading, dataclasses, y patrones de dise√±o
"""

import asyncio
import time
import re
import math
import json
from typing import Dict, List, Tuple, Generator, Callable, Any, Optional, Union
from dataclasses import dataclass, field, asdict
from functools import wraps, lru_cache, cached_property, partial, reduce
from collections import Counter, defaultdict, ChainMap
from concurrent.futures import ThreadPoolExecutor, as_completed
import statistics
from abc import ABC, abstractmethod
from enum import Enum, auto
from pathlib import Path
import hashlib


# ============= ENUMERACIONES =============

class ComplexityLevel(Enum):
    """Niveles de complejidad del texto"""
    SIMPLE = auto()
    MEDIUM = auto()
    COMPLEX = auto()
    ADVANCED = auto()


class AnalysisType(Enum):
    """Tipos de an√°lisis disponibles"""
    FREQUENCY = auto()
    SENTIMENT = auto()
    STRUCTURAL = auto()
    READABILITY = auto()
    STATISTICAL = auto()


# ============= DECORADORES AVANZADOS =============

def timing_decorator(func: Callable) -> Callable:
    """Decorador que mide el tiempo de ejecuci√≥n"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.perf_counter()
        result = func(*args, **kwargs)
        end = time.perf_counter()
        print(f"‚è±Ô∏è  {func.__name__} ejecutado en {end - start:.4f} segundos")
        return result
    return wrapper


def cache_results(max_size: int = 100):
    """Decorador de cach√© con l√≠mite de tama√±o"""
    def decorator(func: Callable) -> Callable:
        cache = {}
        cache_order = []
        
        @wraps(func)
        def wrapper(*args):
            if args in cache:
                return cache[args]
            
            result = func(*args)
            
            if len(cache) >= max_size:
                oldest = cache_order.pop(0)
                del cache[oldest]
            
            cache[args] = result
            cache_order.append(args)
            return result
        
        wrapper.cache = cache
        wrapper.clear_cache = lambda: (cache.clear(), cache_order.clear())
        return wrapper
    return decorator


def retry(max_attempts: int = 3, delay: float = 1.0):
    """Decorador que reintenta la funci√≥n en caso de error"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_attempts - 1:
                        raise
                    print(f"‚ö†Ô∏è  Intento {attempt + 1} fall√≥: {e}. Reintentando...")
                    time.sleep(delay)
        return wrapper
    return decorator


def log_execution(func: Callable) -> Callable:
    """Decorador que registra la ejecuci√≥n de funciones"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        print(f"üìù Ejecutando: {func.__name__}")
        result = func(*args, **kwargs)
        print(f"‚úì  {func.__name__} completado")
        return result
    return wrapper


def memoize_with_expiry(expiry_seconds: int = 60):
    """Decorador de memoization con expiraci√≥n temporal"""
    def decorator(func: Callable) -> Callable:
        cache = {}
        
        @wraps(func)
        def wrapper(*args):
            current_time = time.time()
            cache_key = args
            
            if cache_key in cache:
                result, timestamp = cache[cache_key]
                if current_time - timestamp < expiry_seconds:
                    return result
            
            result = func(*args)
            cache[cache_key] = (result, current_time)
            return result
        
        wrapper.cache = cache
        return wrapper
    return decorator


# ============= DESCRIPTOR PATTERN =============

class ValidatedString:
    """Descriptor que valida strings"""
    def __init__(self, min_length: int = 0, max_length: int = 1000):
        self.min_length = min_length
        self.max_length = max_length
        self.name = None
    
    def __set_name__(self, owner, name):
        self.name = f'_{name}'
    
    def __get__(self, obj, objtype=None):
        if obj is None:
            return self
        return getattr(obj, self.name, '')
    
    def __set__(self, obj, value):
        if not isinstance(value, str):
            raise TypeError(f"Expected string, got {type(value)}")
        if not self.min_length <= len(value) <= self.max_length:
            raise ValueError(f"String length must be between {self.min_length} and {self.max_length}")
        setattr(obj, self.name, value)


class PositiveNumber:
    """Descriptor que valida n√∫meros positivos"""
    def __init__(self, default: float = 0.0):
        self.default = default
        self.name = None
    
    def __set_name__(self, owner, name):
        self.name = f'_{name}'
    
    def __get__(self, obj, objtype=None):
        if obj is None:
            return self
        return getattr(obj, self.name, self.default)
    
    def __set__(self, obj, value):
        if not isinstance(value, (int, float)):
            raise TypeError(f"Expected number, got {type(value)}")
        if value < 0:
            raise ValueError(f"Value must be positive, got {value}")
        setattr(obj, self.name, value)


# ============= CONTEXT MANAGER =============

class TextAnalysisContext:
    """Context manager para an√°lisis de texto con recursos"""
    def __init__(self, name: str):
        self.name = name
        self.start_time = None
        
    def __enter__(self):
        self.start_time = time.time()
        print(f"\nüîç Iniciando an√°lisis: {self.name}")
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        duration = time.time() - self.start_time
        print(f"‚úÖ An√°lisis completado en {duration:.2f}s")
        if exc_type:
            print(f"‚ùå Error: {exc_val}")
        return False


# ============= DATACLASSES =============

@dataclass
class TextStatistics:
    """
    Estad√≠sticas de texto.
    
    Attributes:
        total_words: N√∫mero total de palabras en el texto
        total_chars: N√∫mero total de caracteres incluyendo espacios
        unique_words: Cantidad de palabras √∫nicas (sin repetici√≥n)
        avg_word_length: Longitud promedio de las palabras
        most_common: Lista de tuplas (palabra, frecuencia) m√°s comunes
        sentiment_score: Puntuaci√≥n de sentimiento (-100 a 100)
        readability_score: Puntuaci√≥n de legibilidad (0-100, Flesch Reading Ease)
        complexity_level: Nivel de complejidad del texto
        lexical_diversity: Ratio de palabras √∫nicas sobre palabras totales (0-1)
        sentence_count: N√∫mero de oraciones en el texto
        text_hash: Hash MD5 del texto (8 caracteres)
    """
    total_words: int = 0
    total_chars: int = 0
    unique_words: int = 0
    avg_word_length: float = 0.0
    most_common: List[Tuple[str, int]] = field(default_factory=list)
    sentiment_score: float = 0.0
    readability_score: float = 0.0
    complexity_level: ComplexityLevel = ComplexityLevel.MEDIUM
    lexical_diversity: float = 0.0
    sentence_count: int = 0
    text_hash: str = ""
    
    def __post_init__(self):
        """Valida los datos despu√©s de la inicializaci√≥n"""
        if self.total_words < 0:
            raise ValueError("total_words debe ser no negativo")
        if self.lexical_diversity < 0 or self.lexical_diversity > 1:
            raise ValueError("lexical_diversity debe estar entre 0 y 1")
    
    def __str__(self) -> str:
        return f"""
üìä Estad√≠sticas del Texto:
   - Palabras totales: {self.total_words}
   - Caracteres: {self.total_chars}
   - Palabras √∫nicas: {self.unique_words}
   - Diversidad l√©xica: {self.lexical_diversity:.2%}
   - Longitud promedio: {self.avg_word_length:.2f}
   - Score de sentimiento: {self.sentiment_score:.2f}
   - Legibilidad: {self.readability_score:.2f}
   - Complejidad: {self.complexity_level.name}
   - Oraciones: {self.sentence_count}
   - Palabras m√°s comunes: {', '.join(f'{w}({c})' for w, c in self.most_common[:5])}
"""
    
    def to_dict(self) -> Dict[str, Any]:
        """Convierte a diccionario"""
        data = asdict(self)
        data['complexity_level'] = self.complexity_level.name
        return data
    
    def to_json(self) -> str:
        """Convierte a JSON"""
        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)


# ============= STRATEGY PATTERN CON ABC =============

class AnalysisStrategy(ABC):
    """Estrategia abstracta de an√°lisis"""
    @abstractmethod
    def analyze(self, text: str) -> Dict[str, Any]:
        pass


class FrequencyAnalysis(AnalysisStrategy):
    """An√°lisis de frecuencia de palabras"""
    def analyze(self, text: str) -> Dict[str, Any]:
        words = re.findall(r'\b\w+\b', text.lower())
        return {
            'word_count': Counter(words),
            'total_words': len(words),
            'unique_words': len(set(words))
        }


class SentimentAnalysis(AnalysisStrategy):
    """
    An√°lisis de sentimiento mejorado con categor√≠as emocionales.
    Detecta emociones espec√≠ficas m√°s all√° de positivo/negativo.
    """
    POSITIVE_WORDS = {
        'bueno', 'excelente', 'genial', 'incre√≠ble', 'feliz', 'amor', 'perfecto',
        'fant√°stico', 'maravilloso', 'espectacular', 'estupendo', 'brillante',
        'positivo', 'alegre', 'exitoso', '√©xito', 'victoria', 'ganar'
    }
    NEGATIVE_WORDS = {
        'malo', 'terrible', 'horrible', 'triste', 'odio', 'error', 'problema',
        'p√©simo', 'deficiente', 'fracaso', 'negativo', 'desastre', 'fallo',
        'in√∫til', 'dif√≠cil', 'complicado', 'preocupante', 'crisis'
    }
    ENTHUSIASM_WORDS = {
        'incre√≠ble', 'asombroso', 'impresionante', 'wow', 'guau', 'genial',
        'extraordinario', 'fascinante', 'emocionante'
    }
    NEUTRAL_WORDS = {
        'normal', 'regular', 'est√°ndar', 'com√∫n', 't√≠pico', 'habitual'
    }
    
    def analyze(self, text: str) -> Dict[str, Any]:
        """Analiza el sentimiento y emociones del texto"""
        words = set(re.findall(r'\b\w+\b', text.lower()))
        
        if not words:
            return {
                'sentiment_score': 0.0,
                'positive_words': 0,
                'negative_words': 0,
                'enthusiasm_level': 0,
                'neutrality': 0,
                'emotion': 'neutral'
            }
        
        positive = len(words & self.POSITIVE_WORDS)
        negative = len(words & self.NEGATIVE_WORDS)
        enthusiasm = len(words & self.ENTHUSIASM_WORDS)
        neutral = len(words & self.NEUTRAL_WORDS)
        
        # Calcular score ponderado
        score = ((positive * 1.5 + enthusiasm * 2) - (negative * 1.5)) / max(len(words), 1) * 100
        
        # Determinar emoci√≥n dominante
        if enthusiasm > 1:
            emotion = 'entusiasta'
        elif positive > negative:
            emotion = 'positivo'
        elif negative > positive:
            emotion = 'negativo'
        else:
            emotion = 'neutral'
        
        return {
            'sentiment_score': round(score, 2),
            'positive_words': positive,
            'negative_words': negative,
            'enthusiasm_level': enthusiasm,
            'neutrality': neutral,
            'emotion': emotion
        }


class StructuralAnalysis(AnalysisStrategy):
    """An√°lisis estructural del texto"""
    def analyze(self, text: str) -> Dict[str, Any]:
        sentences = re.split(r'[.!?]+', text)
        paragraphs = text.split('\n\n')
        
        return {
            'sentence_count': len([s for s in sentences if s.strip()]),
            'paragraph_count': len([p for p in paragraphs if p.strip()]),
            'avg_sentence_length': statistics.mean([len(s.split()) for s in sentences if s.strip()] or [0])
        }


class ReadabilityAnalysis(AnalysisStrategy):
    """An√°lisis de legibilidad (√≠ndice Flesch simplificado)"""
    def analyze(self, text: str) -> Dict[str, Any]:
        words = re.findall(r'\b\w+\b', text)
        sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]
        syllables = sum(self._count_syllables(word) for word in words)
        
        if not sentences or not words:
            return {'readability_score': 0.0, 'complexity_level': ComplexityLevel.SIMPLE}
        
        avg_words_per_sentence = len(words) / len(sentences)
        avg_syllables_per_word = syllables / len(words)
        
        # F√≥rmula Flesch Reading Ease simplificada
        score = 206.835 - 1.015 * avg_words_per_sentence - 84.6 * avg_syllables_per_word
        score = max(0, min(100, score))  # Limitar entre 0-100
        
        # Determinar nivel de complejidad
        if score >= 80:
            complexity = ComplexityLevel.SIMPLE
        elif score >= 60:
            complexity = ComplexityLevel.MEDIUM
        elif score >= 40:
            complexity = ComplexityLevel.COMPLEX
        else:
            complexity = ComplexityLevel.ADVANCED
        
        return {
            'readability_score': score,
            'complexity_level': complexity,
            'avg_syllables_per_word': avg_syllables_per_word,
            'avg_words_per_sentence': avg_words_per_sentence
        }
    
    def _count_syllables(self, word: str) -> int:
        """Cuenta s√≠labas (aproximaci√≥n simple)"""
        word = word.lower()
        vowels = 'aeiou√°√©√≠√≥√∫√º'
        count = 0
        previous_was_vowel = False
        
        for char in word:
            is_vowel = char in vowels
            if is_vowel and not previous_was_vowel:
                count += 1
            previous_was_vowel = is_vowel
        
        return max(1, count)


class StatisticalAnalysis(AnalysisStrategy):
    """An√°lisis estad√≠stico avanzado"""
    def analyze(self, text: str) -> Dict[str, Any]:
        words = re.findall(r'\b\w+\b', text)
        word_lengths = [len(w) for w in words]
        
        if not word_lengths:
            return {}
        
        return {
            'median_word_length': statistics.median(word_lengths),
            'mode_word_length': statistics.mode(word_lengths) if word_lengths else 0,
            'stdev_word_length': statistics.stdev(word_lengths) if len(word_lengths) > 1 else 0,
            'min_word_length': min(word_lengths),
            'max_word_length': max(word_lengths),
            'lexical_diversity': len(set(words)) / len(words) if words else 0
        }


class KeywordAnalysis(AnalysisStrategy):
    """
    An√°lisis de palabras clave usando TF-IDF simplificado.
    Identifica las palabras m√°s importantes del texto.
    """
    # Palabras vac√≠as comunes en espa√±ol
    STOP_WORDS = {
        'el', 'la', 'de', 'que', 'y', 'a', 'en', 'un', 'ser', 'se', 'no', 'haber',
        'por', 'con', 'su', 'para', 'como', 'estar', 'tener', 'le', 'lo', 'todo',
        'pero', 'm√°s', 'hacer', 'o', 'poder', 'decir', 'este', 'ir', 'otro', 'ese',
        'si', 'me', 'ya', 'ver', 'porque', 'dar', 'cuando', '√©l', 'muy', 'sin',
        'vez', 'mucho', 'saber', 'qu√©', 'sobre', 'mi', 'alguno', 'mismo', 'yo',
        'tambi√©n', 'hasta', 'a√±o', 'dos', 'querer', 'entre', 'as√≠', 'primero',
        'desde', 'grande', 'eso', 'ni', 'nos', 'llegar', 'pasar', 'tiempo'
    }
    
    def analyze(self, text: str) -> Dict[str, Any]:
        """Extrae palabras clave del texto"""
        words = [w.lower() for w in re.findall(r'\b\w+\b', text)]
        
        # Filtrar palabras vac√≠as y palabras cortas
        keywords = [w for w in words if w not in self.STOP_WORDS and len(w) > 3]
        
        if not keywords:
            return {'keywords': [], 'keyword_density': 0.0}
        
        # Calcular frecuencias
        keyword_freq = Counter(keywords)
        total_keywords = len(keywords)
        
        # Palabras clave con su densidad
        top_keywords = [
            (word, count, count / total_keywords * 100) 
            for word, count in keyword_freq.most_common(10)
        ]
        
        return {
            'keywords': top_keywords,
            'keyword_density': len(keywords) / len(words) * 100 if words else 0,
            'unique_keywords': len(set(keywords))
        }


# ============= GENERADORES =============

def word_generator(text: str) -> Generator[str, None, None]:
    """
    Generador lazy de palabras optimizado.
    
    Args:
        text: Texto del cual extraer palabras
        
    Yields:
        Palabras individuales en min√∫sculas
        
    Example:
        >>> list(word_generator("Hola Mundo"))
        ['hola', 'mundo']
    """
    if not text:
        return
    
    for word in re.finditer(r'\b\w+\b', text.lower()):
        word_text = word.group()
        # Filtrar palabras de un solo car√°cter si son n√∫meros
        if len(word_text) > 1 or not word_text.isdigit():
            yield word_text


def ngram_generator(text: str, n: int = 2) -> Generator[Tuple[str, ...], None, None]:
    """Generador de n-gramas"""
    words = list(word_generator(text))
    for i in range(len(words) - n + 1):
        yield tuple(words[i:i + n])


def sliding_window(iterable, window_size: int = 3) -> Generator[List, None, None]:
    """Generador de ventana deslizante"""
    from collections import deque
    window = deque(maxlen=window_size)
    
    for item in iterable:
        window.append(item)
        if len(window) == window_size:
            yield list(window)


# ============= OBSERVER PATTERN =============

class AnalysisObserver(ABC):
    """Observador abstracto para an√°lisis"""
    @abstractmethod
    def update(self, event_type: str, data: Any):
        pass


class ConsoleObserver(AnalysisObserver):
    """Observador que imprime en consola"""
    def update(self, event_type: str, data: Any):
        print(f"üîî Evento: {event_type} | Datos: {data}")


class StatisticsObserver(AnalysisObserver):
    """Observador que recolecta estad√≠sticas"""
    def __init__(self):
        self.events: List[Tuple[str, Any]] = []
    
    def update(self, event_type: str, data: Any):
        self.events.append((event_type, data))
    
    def get_summary(self) -> Dict[str, int]:
        return Counter(event for event, _ in self.events)


class Observable:
    """Clase observable que notifica a observadores"""
    def __init__(self):
        self._observers: List[AnalysisObserver] = []
    
    def attach(self, observer: AnalysisObserver):
        if observer not in self._observers:
            self._observers.append(observer)
    
    def detach(self, observer: AnalysisObserver):
        if observer in self._observers:
            self._observers.remove(observer)
    
    def notify(self, event_type: str, data: Any):
        for observer in self._observers:
            observer.update(event_type, data)


# ============= CHAIN OF RESPONSIBILITY PATTERN =============

class TextHandler(ABC):
    """Handler abstracto para cadena de responsabilidad"""
    def __init__(self):
        self._next_handler: Optional[TextHandler] = None
    
    def set_next(self, handler: 'TextHandler') -> 'TextHandler':
        self._next_handler = handler
        return handler
    
    @abstractmethod
    def handle(self, text: str) -> str:
        if self._next_handler:
            return self._next_handler.handle(text)
        return text


class CleanSpacesHandler(TextHandler):
    """Limpia espacios m√∫ltiples"""
    def handle(self, text: str) -> str:
        text = re.sub(r'\s+', ' ', text).strip()
        return super().handle(text)


class RemoveSpecialCharsHandler(TextHandler):
    """Remueve caracteres especiales"""
    def handle(self, text: str) -> str:
        text = re.sub(r'[^\w\s.!?,;-]', '', text)
        return super().handle(text)


class LowercaseHandler(TextHandler):
    """Convierte a min√∫sculas"""
    def handle(self, text: str) -> str:
        text = text.lower()
        return super().handle(text)


# ============= CLASE PRINCIPAL CON METACLASE =============

class SingletonMeta(type):
    """Metaclase Singleton"""
    _instances = {}
    
    def __call__(cls, *args, **kwargs):
        if cls not in cls._instances:
            cls._instances[cls] = super().__call__(*args, **kwargs)
        return cls._instances[cls]


class TextAnalyzer(Observable, metaclass=SingletonMeta):
    """Analizador de texto avanzado (Singleton + Observable)"""
    
    name = ValidatedString(min_length=1, max_length=100)
    analysis_count = PositiveNumber(default=0)
    
    def __init__(self, name: str = "Analizador Principal"):
        Observable.__init__(self)
        self.name = name
        self.strategies: List[AnalysisStrategy] = []
        self.results_cache = {}
        self._texts_analyzed: List[str] = []
        self.analysis_count = 0
    
    def add_strategy(self, strategy: AnalysisStrategy):
        """A√±ade una estrategia de an√°lisis"""
        self.strategies.append(strategy)
        return self
    
    @property
    def total_analyses(self) -> int:
        """N√∫mero total de an√°lisis realizados"""
        return int(self.analysis_count)
    
    @cached_property
    def supported_analyses(self) -> List[str]:
        """Lista de an√°lisis soportados"""
        return ['frequency', 'sentiment', 'structural', 'readability', 'statistical']
    
    def __len__(self) -> int:
        """Longitud = n√∫mero de textos analizados"""
        return len(self._texts_analyzed)
    
    def __getitem__(self, index: int) -> str:
        """Acceso a textos analizados por √≠ndice"""
        return self._texts_analyzed[index]
    
    def __iter__(self):
        """Iterador sobre textos analizados"""
        return iter(self._texts_analyzed)
    
    def __contains__(self, text: str) -> bool:
        """Verifica si un texto ya fue analizado"""
        return text in self._texts_analyzed
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """
        Obtiene estad√≠sticas sobre el cach√© de resultados.
        
        Returns:
            Diccionario con informaci√≥n del cach√©
        """
        cache = getattr(self.analyze_text, 'cache', {})
        return {
            'cache_size': len(cache),
            'texts_analyzed': len(self._texts_analyzed),
            'total_analyses': self.total_analyses,
            'cache_hit_potential': f"{len(cache) / max(self.total_analyses, 1) * 100:.1f}%"
        }
    
    @timing_decorator
    @cache_results(max_size=50)
    def analyze_text(self, text: str) -> TextStatistics:
        """
        An√°lisis completo del texto.
        
        Args:
            text: El texto a analizar
            
        Returns:
            TextStatistics con las estad√≠sticas completas del texto
            
        Raises:
            ValueError: Si el texto est√° vac√≠o o es None
        """
        if not text or not text.strip():
            raise ValueError("El texto no puede estar vac√≠o")
        
        self.notify('analysis_started', {'text_length': len(text)})
        
        words = list(word_generator(text))
        
        # An√°lisis de frecuencia
        freq_result = FrequencyAnalysis().analyze(text)
        word_count = freq_result['word_count']
        
        # An√°lisis de sentimiento
        sentiment_result = SentimentAnalysis().analyze(text)
        
        # An√°lisis estructural
        structural_result = StructuralAnalysis().analyze(text)
        
        # An√°lisis de legibilidad
        readability_result = ReadabilityAnalysis().analyze(text)
        
        # An√°lisis estad√≠stico
        statistical_result = StatisticalAnalysis().analyze(text)
        
        # An√°lisis de palabras clave
        keyword_result = KeywordAnalysis().analyze(text)
        
        # Calcular hash del texto
        text_hash = hashlib.md5(text.encode()).hexdigest()[:8]
        
        # Calcular longitud promedio
        avg_length = statistics.mean([len(w) for w in words]) if words else 0
        
        # Registrar an√°lisis
        self._texts_analyzed.append(text[:50] + '...' if len(text) > 50 else text)
        self.analysis_count += 1
        
        stats = TextStatistics(
            total_words=len(words),
            total_chars=len(text),
            unique_words=len(set(words)),
            avg_word_length=avg_length,
            most_common=word_count.most_common(10),
            sentiment_score=sentiment_result['sentiment_score'],
            readability_score=readability_result.get('readability_score', 0.0),
            complexity_level=readability_result.get('complexity_level', ComplexityLevel.MEDIUM),
            lexical_diversity=statistical_result.get('lexical_diversity', 0.0),
            sentence_count=structural_result.get('sentence_count', 0),
            text_hash=text_hash
        )
        
        self.notify('analysis_completed', {'stats': stats.to_dict()})
        return stats
    
    async def analyze_async(self, texts: List[str]) -> List[TextStatistics]:
        """An√°lisis as√≠ncrono de m√∫ltiples textos"""
        async def analyze_one(text: str) -> TextStatistics:
            await asyncio.sleep(0.1)  # Simula operaci√≥n I/O
            return self.analyze_text(text)
        
        tasks = [analyze_one(text) for text in texts]
        return await asyncio.gather(*tasks)
    
    def parallel_analysis(self, texts: List[str], max_workers: int = 4) -> List[TextStatistics]:
        """An√°lisis paralelo usando ThreadPool"""
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            results = list(executor.map(self.analyze_text, texts))
        return results
    
    def find_patterns(self, text: str, pattern: str) -> List[str]:
        """Encuentra patrones usando regex"""
        return re.findall(pattern, text, re.IGNORECASE)
    
    def generate_ngrams(self, text: str, n: int = 2) -> Counter:
        """Genera estad√≠sticas de n-gramas"""
        ngrams = list(ngram_generator(text, n))
        return Counter(ngrams)
    
    def preprocess_text(self, text: str, lowercase: bool = True, 
                       remove_special: bool = True) -> str:
        """Preprocesa texto usando Chain of Responsibility"""
        handler = CleanSpacesHandler()
        
        if remove_special:
            handler.set_next(RemoveSpecialCharsHandler())
        
        if lowercase:
            current = handler
            while current._next_handler:
                current = current._next_handler
            current.set_next(LowercaseHandler())
        
        return handler.handle(text)
    
    def compare_texts(self, text1: str, text2: str) -> Dict[str, Any]:
        """Compara dos textos"""
        stats1 = self.analyze_text(text1)
        stats2 = self.analyze_text(text2)
        
        return {
            'similarity_score': self._calculate_similarity(text1, text2),
            'word_diff': stats1.total_words - stats2.total_words,
            'sentiment_diff': stats1.sentiment_score - stats2.sentiment_score,
            'readability_diff': stats1.readability_score - stats2.readability_score
        }
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """Calcula similitud usando Jaccard"""
        words1 = set(word_generator(text1))
        words2 = set(word_generator(text2))
        
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        return intersection / union if union > 0 else 0.0
    
    def export_results(self, filepath: Union[str, Path], format: str = 'json') -> None:
        """
        Exporta resultados a archivo.
        
        Args:
            filepath: Ruta del archivo de destino
            format: Formato de exportaci√≥n ('json' o 'csv')
        """
        data = {
            'analyzer_name': self.name,
            'total_analyses': self.total_analyses,
            'texts_analyzed': self._texts_analyzed
        }
        
        filepath = Path(filepath)
        
        if format.lower() == 'json':
            filepath.write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding='utf-8')
        elif format.lower() == 'csv':
            import csv
            with filepath.open('w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(['Analizador', 'Total An√°lisis', 'Texto'])
                for text in self._texts_analyzed:
                    writer.writerow([self.name, self.total_analyses, text])
        else:
            raise ValueError(f"Formato no soportado: {format}. Use 'json' o 'csv'")
        
        print(f"üíæ Resultados exportados a {filepath} (formato: {format.upper()})")
    
    def batch_analyze(self, texts: List[str], show_progress: bool = True) -> List[TextStatistics]:
        """An√°lisis en lote con barra de progreso"""
        results = []
        total = len(texts)
        
        for i, text in enumerate(texts, 1):
            if show_progress:
                print(f"\rüìä Progreso: {i}/{total} ({i/total*100:.1f}%)", end='')
            results.append(self.analyze_text(text))
        
        if show_progress:
            print()  # Nueva l√≠nea
        
        return results
    
    def clear_cache(self) -> None:
        """
        Limpia el cach√© de resultados de an√°lisis.
        √ötil para liberar memoria o forzar re-an√°lisis.
        """
        if hasattr(self.analyze_text, 'clear_cache'):
            self.analyze_text.clear_cache()
            print("üßπ Cach√© limpiado exitosamente")
        else:
            print("‚ö†Ô∏è  No hay cach√© para limpiar")
    
    def reset_statistics(self) -> None:
        """
        Resetea todas las estad√≠sticas del analizador.
        Mantiene las estrategias pero limpia historial.
        """
        self._texts_analyzed.clear()
        self.analysis_count = 0
        self.clear_cache()
        print("üîÑ Estad√≠sticas reseteadas")


# ============= FUNCIONES DE DEMOSTRACI√ìN =============

@timing_decorator
def demo_basic_analysis():
    """Demostraci√≥n de an√°lisis b√°sico"""
    with TextAnalysisContext("An√°lisis B√°sico"):
        analyzer = TextAnalyzer("Mi Analizador")
        
        # Agregar observadores
        stats_observer = StatisticsObserver()
        analyzer.attach(stats_observer)
        
        sample_text = """
        Python es un lenguaje de programaci√≥n incre√≠ble y poderoso.
        Es excelente para an√°lisis de datos, machine learning y desarrollo web.
        La comunidad de Python es genial y muy activa.
        Aprender Python es una decisi√≥n inteligente para cualquier programador.
        """
        
        stats = analyzer.analyze_text(sample_text)
        print(stats)
        print(f"\nüìà Hash del texto: {stats.text_hash}")
        print(f"üéØ An√°lisis realizados: {analyzer.total_analyses}")
        
        # An√°lisis de n-gramas
        bigrams = analyzer.generate_ngrams(sample_text, 2)
        print(f"\nüî§ Bigramas m√°s comunes:")
        for ngram, count in bigrams.most_common(5):
            print(f"   {' '.join(ngram)}: {count}")
        
        # An√°lisis de palabras clave
        keyword_analysis = KeywordAnalysis().analyze(sample_text)
        print(f"\nüîë Palabras clave detectadas:")
        for word, count, density in keyword_analysis['keywords'][:5]:
            print(f"   {word}: {count} veces ({density:.1f}% densidad)")
        
        # Preprocesamiento
        processed = analyzer.preprocess_text(sample_text)
        print(f"\nüîß Texto preprocesado (primeros 100 caracteres): {processed[:100]}...")
        
        # Resumen de eventos
        print(f"\nüìä Eventos capturados: {stats_observer.get_summary()}")


@timing_decorator
def demo_parallel_analysis():
    """Demostraci√≥n de an√°lisis paralelo"""
    with TextAnalysisContext("An√°lisis Paralelo"):
        analyzer = TextAnalyzer()
        
        texts = [
            "Python es incre√≠ble para ciencia de datos.",
            "Machine learning con Python es genial.",
            "El desarrollo web con Django es excelente.",
            "La sintaxis de Python es limpia y elegante.",
            "Programar en Python es muy productivo."
        ]
        
        results = analyzer.parallel_analysis(texts, max_workers=3)
        
        print(f"\nüìö Analizados {len(results)} textos en paralelo:")
        for i, stat in enumerate(results, 1):
            print(f"\n   Texto {i}: {stat.total_words} palabras, "
                  f"sentimiento: {stat.sentiment_score:.2f}")


async def demo_async_analysis():
    """Demostraci√≥n de an√°lisis as√≠ncrono"""
    print("\nüöÄ Iniciando an√°lisis as√≠ncrono...")
    
    analyzer = TextAnalyzer()
    texts = [
        "Async/await en Python es poderoso.",
        "La programaci√≥n concurrente mejora el rendimiento.",
        "Python 3.11 es m√°s r√°pido que nunca.",
    ]
    
    results = await analyzer.analyze_async(texts)
    print(f"‚úÖ An√°lisis as√≠ncrono completado: {len(results)} textos procesados")


def demo_generators():
    """Demostraci√≥n de generadores"""
    print("\nüîÑ Demostraci√≥n de Generadores:")
    
    text = "Python es genial para programaci√≥n funcional y orientada a objetos"
    
    # Uso de generador con comprensi√≥n
    long_words = (word for word in word_generator(text) if len(word) > 5)
    print(f"   Palabras largas: {list(long_words)}")
    
    # N-gramas
    trigrams = list(ngram_generator(text, 3))
    print(f"   Trigramas encontrados: {len(trigrams)}")
    print(f"   Primeros 3: {trigrams[:3]}")


def demo_decorator_retry():
    """Demostraci√≥n del decorador retry"""
    @retry(max_attempts=3, delay=0.5)
    def unstable_function(should_fail: bool = False):
        if should_fail:
            raise ValueError("Fallo simulado")
        return "¬°√âxito!"
    
    print("\nüîÅ Demostraci√≥n de Retry Decorator:")
    result = unstable_function(False)
    print(f"   Resultado: {result}")


def demo_text_comparison():
    """Demostraci√≥n de comparaci√≥n de textos"""
    print("\nüîç Demostraci√≥n de Comparaci√≥n de Textos:")
    
    analyzer = TextAnalyzer()
    
    text1 = "Python es excelente para ciencia de datos y an√°lisis."
    text2 = "Python es genial para machine learning y an√°lisis de datos."
    
    comparison = analyzer.compare_texts(text1, text2)
    
    print(f"   Texto 1: {text1}")
    print(f"   Texto 2: {text2}")
    print(f"\n   üìä Resultados:")
    print(f"      - Similitud: {comparison['similarity_score']:.2%}")
    print(f"      - Diferencia de palabras: {comparison['word_diff']}")
    print(f"      - Diferencia de sentimiento: {comparison['sentiment_diff']:.2f}")
    print(f"      - Diferencia de legibilidad: {comparison['readability_diff']:.2f}")


def demo_advanced_features():
    """Demostraci√≥n de caracter√≠sticas avanzadas"""
    print("\nüöÄ Demostraci√≥n de Caracter√≠sticas Avanzadas:")
    
    analyzer = TextAnalyzer()
    
    # Usando functools
    from functools import reduce
    texts = [
        "Python es incre√≠ble.",
        "Programar es divertido.",
        "La tecnolog√≠a avanza r√°pido."
    ]
    
    # Combinar todas las palabras
    all_words = reduce(lambda a, b: a + list(word_generator(b)), texts, [])
    print(f"   Total de palabras combinadas: {len(all_words)}")
    
    # Usar m√©todos m√°gicos
    analyzer.analyze_text(texts[0])
    analyzer.analyze_text(texts[1])
    
    print(f"\n   üìö Usando m√©todos m√°gicos:")
    print(f"      - len(analyzer): {len(analyzer)}")
    print(f"      - analyzer[0]: {analyzer[0]}")
    print(f"      - 'Python' in analyzer[0]: {'Python' in analyzer[0]}")
    
    # Sliding window
    words = ['Python', 'es', 'un', 'lenguaje', 'poderoso']
    windows = list(sliding_window(words, 3))
    print(f"\n   ü™ü Ventanas deslizantes (tama√±o 3):")
    for window in windows:
        print(f"      {window}")


def demo_cache_management():
    """Demostraci√≥n de gesti√≥n de cach√©"""
    print("\nüóÑÔ∏è  Demostraci√≥n de Gesti√≥n de Cach√©:")
    
    analyzer = TextAnalyzer()
    
    # Analizar algunos textos
    texts = [
        "Python es un lenguaje incre√≠ble",
        "La programaci√≥n es fascinante",
        "Python es un lenguaje incre√≠ble"  # Repetido para demostrar cach√©
    ]
    
    for text in texts:
        analyzer.analyze_text(text)
    
    # Mostrar estad√≠sticas de cach√©
    cache_stats = analyzer.get_cache_stats()
    print(f"\n   üìä Estad√≠sticas de Cach√©:")
    for key, value in cache_stats.items():
        print(f"      - {key}: {value}")
    
    # Exportar a CSV
    try:
        analyzer.export_results("resultados_test.csv", format='csv')
        print(f"   ‚úÖ Exportaci√≥n a CSV exitosa")
    except Exception as e:
        print(f"   ‚ÑπÔ∏è  Exportaci√≥n omitida: {e}")


def demo_statistics():
    """Demostraci√≥n de estad√≠sticas avanzadas"""
    print("\nüìà Demostraci√≥n de Estad√≠sticas Avanzadas:")
    
    analyzer = TextAnalyzer()
    
    complex_text = """
    La inteligencia artificial est√° revolucionando m√∫ltiples industrias.
    Los algoritmos de aprendizaje autom√°tico procesan enormes cantidades de datos.
    Las redes neuronales profundas imitan el funcionamiento del cerebro humano.
    Esta tecnolog√≠a est√° transformando fundamentalmente nuestra sociedad contempor√°nea.
    """
    
    stats = analyzer.analyze_text(complex_text)
    
    print(f"   üìä Estad√≠sticas detalladas:")
    print(f"      - Diversidad l√©xica: {stats.lexical_diversity:.2%}")
    print(f"      - Nivel de complejidad: {stats.complexity_level.name}")
    print(f"      - Score de legibilidad: {stats.readability_score:.2f}")
    print(f"      - Total de oraciones: {stats.sentence_count}")
    
    # Exportar a JSON
    print(f"\n   üìÑ JSON generado:")
    print(stats.to_json())


# ============= PROGRAMA PRINCIPAL =============

def main():
    """Funci√≥n principal que ejecuta todas las demostraciones"""
    print("=" * 80)
    print("üêç SISTEMA AVANZADO DE AN√ÅLISIS DE TEXTO EN PYTHON üêç")
    print("   Versi√≥n 2.2 - An√°lisis de Emociones + Gesti√≥n de Cach√©")
    print("   Nuevas caracter√≠sticas: Emociones detalladas, exportaci√≥n CSV, cach√© stats")
    print("=" * 80)
    
    # Verificar Singleton
    analyzer1 = TextAnalyzer("Primer Analizador")
    analyzer2 = TextAnalyzer("Segundo Analizador")
    print(f"\nüîç Verificaci√≥n Singleton: {analyzer1 is analyzer2}")
    print(f"   Nombre del analizador: {analyzer1.name}")
    print(f"   An√°lisis soportados: {analyzer1.supported_analyses}")
    
    # Demostraciones
    demo_basic_analysis()
    demo_parallel_analysis()
    demo_generators()
    demo_decorator_retry()
    demo_text_comparison()
    demo_advanced_features()
    demo_cache_management()
    demo_statistics()
    
    # An√°lisis as√≠ncrono
    asyncio.run(demo_async_analysis())
    
    # Resumen final
    print("\n" + "=" * 80)
    print("üìä RESUMEN FINAL:")
    print(f"   Total de an√°lisis realizados: {analyzer1.total_analyses}")
    print(f"   Textos en memoria: {len(analyzer1)}")
    print("‚ú® Todas las demostraciones completadas exitosamente ‚ú®")
    print("=" * 80)


if __name__ == "__main__":
    main()
